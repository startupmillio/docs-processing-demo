# ðŸŽ™ï¸ Audio Processing Demo

A live transcription demo powered by Vosk, WebSockets, Auth0 and OpenAI.

## ðŸ“š API Documentation

Explore the full API via interactive Swagger UI:  
ðŸ‘‰ [API Docs](https://audio-processing-demo.click/docs#/)

> âš ï¸ **Authentication Required**  
> All endpoints require cookie-based Auth0 login. Without proper authentication, requests will return a **401 Unauthorized** error.

---

## ðŸ” Authentication

Login here to get started:  
ðŸ”— [Login Page](https://audio-processing-demo.click/auth/login)

- Auth0-based authentication
- You can create a new account
- Once logged in, you'll be redirected to the main app

Logout when done:  
ðŸ”— [Logout](https://audio-processing-demo.click/auth/logout)

---

## ðŸ§  Main Application

ðŸ”— [Live Transcription App](https://audio-processing-demo.click/)

- Real-time audio transcription via **WebSockets**
- Auto-generates and inserts `meeting-id` in requests
- Transcripts and summaries are fetched based on this ID

> âš ï¸ Frontend is rough â€” built only for demo purposes..

---

## ðŸ“ File Upload

ðŸ”— [Upload Audio File](https://audio-processing-demo.click/upload-file)

- Upload files using a `presigned_link`
- Simple and minimal UI for quick testing

---

## ðŸ“ Transcription Workflow

1. **Start a Transcription Task**  
   `POST` to [`/transcribe/`](https://audio-processing-demo.click/docs/)  
   Submit your `s3key` to initiate transcript & summary generation.

2. **Check Task Status**  
   `GET`:   `/transcribe/task/{task_id}`

3. **Fetch Transcript & Summary**  
   `GET`:   `/transcribe/result/{meeting_id}`

---

## Setting up

### Prerequisites

Before proceeding, ensure you have the following tools installed and configured:

- [Docker](https://www.docker.com/get-started)
- [Docker Compose](https://docs.docker.com/compose/install/)
- [AWS access keys](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html)
- [OpenAi ApiKey](https://platform.openai.com/api-keys)
- [Postgresql](https://www.postgresql.org/) (to run locally)
- [Redis](https://redis.io/) (to run locally)

### Auth0 App settings
1. Go to Auth0 official site and login there
2. Go to the `Applications` tab, `+Create Application` button, choose Regular Web Application. Create one
3. Go to `Settings`. here you'll see your `AUTH0_DOMAIN_NAME`, `AUTH0_CLIENT_ID`, `AUTH0_CLIENT_SECRET`. 
4. In the `Application URIs` section in `Allowed Callback URLs` paste `{ur_domain_name}/auth/callback`, in `Allowed LoaOur URLs` -> `{ur_domain_name}/auth/login`
5. Scroll down to `Advanced settings`. There go to `Grant Types` and add `Authorization code` option
6. Save
7. Go to the `APIs` tab, `+Create API` button, `Identifier` -> `URL_DOMAIN_NAME` -> Save

### Running the app locally

1. Setup a virtualenv using python3.11
2. pip install pipenv
3. pipenv install
4. copy `.secrets.toml.example` as `.secrets.toml` and replace placeholders with your variables
5. `alembic upgrade head`
6. `python -m main.py`
7. `celery -A tasks.transcribe_audio worker --loglevel=info`
8. `localhost:8000/docs`

### Running as docker-compose 
1. in `settings.toml` replace empty values in the [default] section with your variables
2. `docker-compose up --build`
3. `localhost:8000/docs`


## ðŸš€ Future Improvements

- **Tests!!!**
- **Speaker Recognition** â€” Not just what was said, but *who* said it
- **Multilanguage Support**
- **Switch to Whisper** â€” Test Whisper for file-based transcription, compare results
- **Fine-tuned Summarization Agent** â€” Current summary is generated via a LangGraph agent using a single tool. Improvements can include:
  - Action/Topic/Tag Extraction
  - Multilanguage Support via LangChain tools
- **WebRTC Support** â€” Enable in-browser audio capture for a smoother real-time experience
"""
- **GPU Acceleration** â€” Evaluate performance vs. cost
- **Explore Alternatives** â€” Investigate other open-source solutions for real-time speech recognition
- **Vosk Enhancements** â€” Improve text output with configs (inspired by [nerd-dictation](https://github.com/ideasman42/nerd-dictation))
- **Chunked Processing** â€” Break audio into chunks using Celery tasks
- **Refresh Token Handling**
- **User Management** â€” Roles, permissions, and beyond
- **Realtime Audio Save** â€” Continuous S3 upload of live speech
- **User-Specific Recognition** â€” Save users and identify them in audio files (Closed-system, lots of work)
