# ğŸ™ï¸ Audio Processing Demo

A live transcription demo powered by Vosk, WebSockets, Auth0 and OpenAI.

## ğŸ“š API Documentation

Explore the full API via interactive Swagger UI:  
ğŸ‘‰ [API Docs](https://audio-processing-demo.click/docs#/)

> âš ï¸ **Authentication Required**  
> All endpoints require cookie-based Auth0 login. Without proper authentication, requests will return a **401 Unauthorized** error.

---

## ğŸ” Authentication

Login here to get started:  
ğŸ”— [Login Page](https://audio-processing-demo.click/auth/login)

- Auth0-based authentication
- You can create a new account
- Once logged in, you'll be redirected to the main app

Logout when done:  
ğŸ”— [Logout](https://audio-processing-demo.click/auth/logout)

---

## ğŸ§  Main Application

ğŸ”— [Live Transcription App](https://audio-processing-demo.click/)

- Real-time audio transcription via **WebSockets**
- Auto-generates and inserts `meeting-id` in requests
- Transcripts and summaries are fetched based on this ID

> âš ï¸ Frontend is rough â€” built only for demo purposes..

---

## ğŸ“ File Upload

ğŸ”— [Upload Audio File](https://audio-processing-demo.click/upload-file)

- Upload files using a `presigned_link`
- Simple and minimal UI for quick testing

---

## ğŸ“ Transcription Workflow

1. **Start a Transcription Task**  
   `POST` to [`/transcribe/`](https://audio-processing-demo.click/docs/)  
   Submit your `s3key` to initiate transcript & summary generation.

2. **Check Task Status**  
   `GET`:   `/transcribe/task/{task_id}`

3. **Fetch Transcript & Summary**  
   `GET`:   `/transcribe/result/{meeting_id}`

---

## ğŸš€ Future Improvements

- **Tests!!!**
- **Speaker Recognition** â€” Not just what was said, but *who* said it
- **Multilanguage Support**
- **Switch to Whisper** â€” Test Whisper for file-based transcription, compare results
- **Fine-tuned Summarization Agent** â€” Current summary is generated via a LangGraph agent using a single tool. Improvements can include:
  - Action/Topic/Tag Extraction
  - Multilanguage Support via LangChain tools
- **WebRTC Support** â€” Enable in-browser audio capture for a smoother real-time experience
"""
- **GPU Acceleration** â€” Evaluate performance vs. cost
- **Explore Alternatives** â€” Investigate other open-source solutions for real-time speech recognition
- **Vosk Enhancements** â€” Improve text output with configs (inspired by [nerd-dictation](https://github.com/ideasman42/nerd-dictation))
- **Chunked Processing** â€” Break audio into chunks using Celery tasks
- **Refresh Token Handling**
- **User Management** â€” Roles, permissions, and beyond
- **Realtime Audio Save** â€” Continuous S3 upload of live speech
- **User-Specific Recognition** â€” Save users and identify them in audio files (Closed-system, lots of work)
